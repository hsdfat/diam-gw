name: Gateway CI/CD with Performance Testing

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]
  workflow_dispatch:
    inputs:
      performance_duration:
        description: 'Performance test duration (seconds)'
        required: false
        default: '60'
      performance_rate:
        description: 'Target request rate (req/s)'
        required: false
        default: '1000'

env:
  GO_VERSION: '1.23'

jobs:
  build:
    name: Build and Unit Test
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ env.GO_VERSION }}
        check-latest: true

    - name: Cache Go modules
      uses: actions/cache@v4
      with:
        path: |
          ~/go/pkg/mod
          ~/.cache/go-build
        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
        restore-keys: |
          ${{ runner.os }}-go-

    - name: Download dependencies
      run: go mod download

    - name: Verify Go installation
      run: |
        go version
        go env GOROOT
        go env GOTOOLDIR

    - name: Run unit tests
      run: go test -v -race -coverprofile=coverage.txt -covermode=atomic ./...

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.txt
        flags: unittests
        name: codecov-umbrella

    - name: Build Gateway
      run: CGO_ENABLED=0 GOOS=linux go build -o bin/gateway ./cmd/gateway/main.go

    - name: Build DRA Simulator
      run: CGO_ENABLED=0 GOOS=linux go build -o bin/dra ./simulator/dra/*.go

    - name: Build App Simulator
      run: CGO_ENABLED=0 GOOS=linux go build -o bin/app ./simulator/app/*.go

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: binaries
        path: bin/
        retention-days: 1

  multi-app-test:
    name: Multi-Application Interface Test
    runs-on: ubuntu-latest
    needs: build

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker images
      run: |
        docker build -f Dockerfile.gateway -t diam-gw-gateway:latest .
        docker build -f Dockerfile.dra -t diam-gw-dra:latest .
        docker build -f Dockerfile.app -t diam-gw-app:latest .

    - name: Run Multi-App Test
      run: |
        cd tests/multi-app
        chmod +x test-multi-app-podman.sh
        # Replace podman-compose with docker-compose for GitHub Actions
        sed -i 's/podman-compose/docker compose/g' test-multi-app-podman.sh
        sed -i 's/podman /docker /g' test-multi-app-podman.sh
        ./test-multi-app-podman.sh

    - name: Collect test logs
      if: always()
      run: |
        mkdir -p test-logs
        docker logs gateway > test-logs/gateway.log 2>&1 || true
        docker logs dra > test-logs/dra.log 2>&1 || true
        docker logs app1-s13 > test-logs/app1-s13.log 2>&1 || true
        docker logs app2-s6a > test-logs/app2-s6a.log 2>&1 || true
        docker logs app3-multi > test-logs/app3-multi.log 2>&1 || true
        docker logs app4-gx > test-logs/app4-gx.log 2>&1 || true

    - name: Upload test logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: multi-app-test-logs
        path: test-logs/
        retention-days: 7

  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: build

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker images
      run: |
        docker build -f Dockerfile.gateway -t diam-gw-gateway:latest .
        docker build -f Dockerfile.dra -t diam-gw-dra:latest .
        docker build -f Dockerfile.app -t diam-gw-app:latest .

    - name: Run Performance Test
      id: perf_test
      run: |
        cd tests/performance
        chmod +x test-performance-podman.sh

        # Set test parameters
        DURATION=${{ github.event.inputs.performance_duration || '60' }}
        RATE=${{ github.event.inputs.performance_rate || '1000' }}

        # Replace podman-compose with docker-compose for GitHub Actions
        sed -i 's/podman-compose/docker compose/g' test-performance-podman.sh
        sed -i 's/podman /docker /g' test-performance-podman.sh

        # Run test with output capture
        export DURATION=$DURATION
        export TARGET_RATE=$RATE
        ./test-performance-podman.sh | tee performance-output.txt

        # Store exit code
        echo "test_result=$?" >> $GITHUB_OUTPUT

    - name: Extract Performance Metrics
      id: metrics
      run: |
        cd tests/performance

        # Extract metrics from test output
        ROUTING_ERRORS=$(grep "Routing Errors:" performance-output.txt | awk '{print $3}' || echo "0")
        AFFINITY_VIOLATIONS=$(grep "Affinity Violations:" performance-output.txt | awk '{print $3}' || echo "0")
        CONNECTION_ERRORS=$(grep "Connection Errors:" performance-output.txt | awk '{print $3}' || echo "0")

        # Try to extract throughput
        TOTAL_MSG=$(grep "Total Messages:" performance-output.txt | awk '{print $3}' || echo "N/A")
        ACTUAL_RATE=$(grep "Actual Throughput:" performance-output.txt | awk '{print $3}' || echo "N/A")
        TARGET_RATE=$(grep "Target Throughput:" performance-output.txt | awk '{print $3}' || echo "N/A")

        # Extract grade
        GRADE=$(grep "Grade:" performance-output.txt | awk '{print $3}' || echo "Unknown")

        # Output to GitHub
        echo "routing_errors=$ROUTING_ERRORS" >> $GITHUB_OUTPUT
        echo "affinity_violations=$AFFINITY_VIOLATIONS" >> $GITHUB_OUTPUT
        echo "connection_errors=$CONNECTION_ERRORS" >> $GITHUB_OUTPUT
        echo "total_messages=$TOTAL_MSG" >> $GITHUB_OUTPUT
        echo "actual_rate=$ACTUAL_RATE" >> $GITHUB_OUTPUT
        echo "target_rate=$TARGET_RATE" >> $GITHUB_OUTPUT
        echo "grade=$GRADE" >> $GITHUB_OUTPUT

    - name: Create Performance Report
      run: |
        cat > performance-report.md << 'EOF'
        # Performance Test Report

        ## Test Configuration
        - **Duration**: ${{ github.event.inputs.performance_duration || '60' }}s
        - **Target Rate**: ${{ github.event.inputs.performance_rate || '1000' }} req/s
        - **Branch**: ${{ github.ref_name }}
        - **Commit**: ${{ github.sha }}
        - **Run ID**: ${{ github.run_id }}

        ## Results

        ### Throughput
        - **Total Messages**: ${{ steps.metrics.outputs.total_messages }}
        - **Actual Rate**: ${{ steps.metrics.outputs.actual_rate }} req/s
        - **Target Rate**: ${{ steps.metrics.outputs.target_rate }} req/s

        ### Error Analysis
        - **Routing Errors**: ${{ steps.metrics.outputs.routing_errors }}
        - **Affinity Violations**: ${{ steps.metrics.outputs.affinity_violations }}
        - **Connection Errors**: ${{ steps.metrics.outputs.connection_errors }}

        ### Overall Grade
        **${{ steps.metrics.outputs.grade }}**

        ## Interface Distribution
        - S13: 40% (Equipment Check)
        - S6a: 40% (Authentication)
        - Gx: 20% (Policy Control)

        ## Test Setup
        - 1 DRA (Load Generator)
        - 1 Gateway
        - 12 Applications (4 per interface)
        - Total: 24 concurrent connections

        ---

        *Generated on: $(date -u '+%Y-%m-%d %H:%M:%S UTC')*
        EOF

    - name: Upload Performance Report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: performance-report.md
        retention-days: 30

    - name: Collect performance logs
      if: always()
      run: |
        mkdir -p perf-logs
        docker logs gateway-perf > perf-logs/gateway.log 2>&1 || true
        docker logs dra-perf > perf-logs/dra.log 2>&1 || true
        for i in {1..4}; do
          docker logs app-s13-$i > perf-logs/app-s13-$i.log 2>&1 || true
          docker logs app-s6a-$i > perf-logs/app-s6a-$i.log 2>&1 || true
          docker logs app-gx-$i > perf-logs/app-gx-$i.log 2>&1 || true
        done

    - name: Upload performance logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-logs
        path: perf-logs/
        retention-days: 7

    - name: Comment PR with Performance Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('performance-report.md', 'utf8');

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });

    - name: Check Performance Thresholds
      run: |
        ROUTING_ERRORS=${{ steps.metrics.outputs.routing_errors }}
        ACTUAL_RATE=${{ steps.metrics.outputs.actual_rate }}
        TARGET_RATE=${{ steps.metrics.outputs.target_rate }}

        # Fail if there are any routing errors
        if [ "$ROUTING_ERRORS" != "0" ] && [ "$ROUTING_ERRORS" != "N/A" ]; then
          echo "❌ FAILED: Routing errors detected: $ROUTING_ERRORS"
          exit 1
        fi

        # Warn if throughput is below 95% of target (but don't fail)
        if [ "$ACTUAL_RATE" != "N/A" ] && [ "$TARGET_RATE" != "N/A" ]; then
          THRESHOLD=$((TARGET_RATE * 95 / 100))
          if [ "$ACTUAL_RATE" -lt "$THRESHOLD" ]; then
            echo "⚠️  WARNING: Throughput below 95% threshold"
            echo "   Actual: $ACTUAL_RATE req/s, Expected: >=$THRESHOLD req/s"
          fi
        fi

        echo "✅ Performance test passed"

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [build, multi-app-test, performance-test]
    if: always()

    steps:
    - name: Download performance report
      uses: actions/download-artifact@v4
      with:
        name: performance-report
      continue-on-error: true

    - name: Create Job Summary
      run: |
        echo "# Diameter Gateway CI/CD Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Job Status" >> $GITHUB_STEP_SUMMARY
        echo "- Build: ${{ needs.build.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Multi-App Test: ${{ needs.multi-app-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Test: ${{ needs.performance-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f performance-report.md ]; then
          echo "## Performance Results" >> $GITHUB_STEP_SUMMARY
          cat performance-report.md >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "View full logs in the Actions artifacts" >> $GITHUB_STEP_SUMMARY
