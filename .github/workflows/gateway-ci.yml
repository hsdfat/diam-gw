name: Gateway CI/CD with Performance Testing

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]
  workflow_dispatch:
    inputs:
      performance_duration:
        description: 'Performance test duration (seconds)'
        required: false
        default: '60'
      performance_rate:
        description: 'Target request rate (req/s)'
        required: false
        default: '1000'

env:
  GO_VERSION: '1.23'

jobs:
  build:
    name: Build and Unit Test
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ env.GO_VERSION }}
        check-latest: true

    - name: Cache Go modules
      uses: actions/cache@v4
      with:
        path: |
          ~/go/pkg/mod
          ~/.cache/go-build
        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
        restore-keys: |
          ${{ runner.os }}-go-

    - name: Download dependencies
      run: go mod download

    - name: Verify Go installation
      run: |
        go version
        go env GOROOT
        go env GOTOOLDIR

    - name: Run unit tests
      run: |
        # Run tests excluding main packages (simulator, examples, cmd)
        go test -v -race -coverprofile=coverage.txt -covermode=atomic \
          $(go list ./... | grep -v -E '/(simulator|examples|cmd)/')

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.txt
        flags: unittests
        name: codecov-umbrella

    - name: Build Gateway
      run: CGO_ENABLED=0 GOOS=linux go build -o bin/gateway ./cmd/gateway/main.go

    - name: Build DRA Simulator
      run: CGO_ENABLED=0 GOOS=linux go build -o bin/dra ./simulator/dra/*.go

    - name: Build App Simulator
      run: CGO_ENABLED=0 GOOS=linux go build -o bin/app ./simulator/app/*.go

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: binaries
        path: bin/
        retention-days: 1

  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: build

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker images
      run: |
        docker build -f Dockerfile.gateway -t diam-gw-gateway:latest .
        docker build -f Dockerfile.dra -t diam-gw-dra:latest .
        docker build -f Dockerfile.app -t diam-gw-app:latest .

    - name: Run Performance Test
      id: perf_test
      run: |
        cd tests/performance
        chmod +x test-performance-podman.sh

        # Set test parameters
        DURATION=${{ github.event.inputs.performance_duration || '60' }}
        RATE=${{ github.event.inputs.performance_rate || '1000' }}

        # Replace podman-compose with docker-compose for GitHub Actions
        sed -i 's/podman-compose/docker compose/g' test-performance-podman.sh
        sed -i 's/podman /docker /g' test-performance-podman.sh

        # Run test with output capture
        export DURATION=$DURATION
        export TARGET_RATE=$RATE
        ./test-performance-podman.sh | tee performance-output.txt

        # Store exit code
        echo "test_result=$?" >> $GITHUB_OUTPUT

    - name: Extract Performance Metrics
      id: metrics
      run: |
        cd tests/performance

        # Extract error metrics
        ROUTING_ERRORS=$(grep "Routing Errors:" performance-output.txt | awk '{print $3}' || echo "0")
        AFFINITY_VIOLATIONS=$(grep "Affinity Violations:" performance-output.txt | awk '{print $3}' || echo "0")
        CONNECTION_ERRORS=$(grep "Connection Errors:" performance-output.txt | awk '{print $3}' || echo "0")

        # Extract application message counts (new format)
        APP_TO_DRA=$(grep "App to DRA:" performance-output.txt | awk '{print $4}' || echo "0")
        DRA_TO_APP=$(grep "DRA to App:" performance-output.txt | awk '{print $4}' || echo "0")

        # Calculate total application messages
        TOTAL_APP_MSG=$((APP_TO_DRA + DRA_TO_APP))

        # Extract throughput
        ACTUAL_RATE=$(grep "Actual Throughput:" performance-output.txt | grep -oE '[0-9]+' | head -1 || echo "0")
        TARGET_RATE=$(grep "Target Throughput:" performance-output.txt | grep -oE '[0-9]+' | head -1 || echo "1000")

        # Calculate success percentage
        if [ "$TARGET_RATE" -gt 0 ]; then
          SUCCESS_PCT=$((ACTUAL_RATE * 100 / TARGET_RATE))
        else
          SUCCESS_PCT=0
        fi

        # Determine grade
        if [ "$ROUTING_ERRORS" -eq 0 ] && [ "$CONNECTION_ERRORS" -eq 0 ]; then
          if [ "$SUCCESS_PCT" -ge 95 ]; then
            GRADE="A"
            GRADE_EMOJI="ðŸŸ¢"
          elif [ "$SUCCESS_PCT" -ge 80 ]; then
            GRADE="B"
            GRADE_EMOJI="ðŸŸ¡"
          else
            GRADE="C"
            GRADE_EMOJI="ðŸŸ "
          fi
        else
          GRADE="F"
          GRADE_EMOJI="ðŸ”´"
        fi

        # Output to GitHub
        echo "routing_errors=$ROUTING_ERRORS" >> $GITHUB_OUTPUT
        echo "affinity_violations=$AFFINITY_VIOLATIONS" >> $GITHUB_OUTPUT
        echo "connection_errors=$CONNECTION_ERRORS" >> $GITHUB_OUTPUT
        echo "app_to_dra=$APP_TO_DRA" >> $GITHUB_OUTPUT
        echo "dra_to_app=$DRA_TO_APP" >> $GITHUB_OUTPUT
        echo "total_messages=$TOTAL_APP_MSG" >> $GITHUB_OUTPUT
        echo "actual_rate=$ACTUAL_RATE" >> $GITHUB_OUTPUT
        echo "target_rate=$TARGET_RATE" >> $GITHUB_OUTPUT
        echo "success_pct=$SUCCESS_PCT" >> $GITHUB_OUTPUT
        echo "grade=$GRADE" >> $GITHUB_OUTPUT
        echo "grade_emoji=$GRADE_EMOJI" >> $GITHUB_OUTPUT

    - name: Create Performance Report
      run: |
        cat > performance-report.md << EOF
        # ${{ steps.metrics.outputs.grade_emoji }} Performance Test Report - Grade: ${{ steps.metrics.outputs.grade }}

        ## Test Configuration
        | Parameter | Value |
        |-----------|-------|
        | **Duration** | ${{ github.event.inputs.performance_duration || '60' }}s |
        | **Target Rate** | ${{ github.event.inputs.performance_rate || '1000' }} req/s |
        | **Branch** | ${{ github.ref_name }} |
        | **Commit** | `${{ github.sha }}` |
        | **Run ID** | [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) |
        | **Triggered By** | ${{ github.event_name }} |

        ## Results Summary

        ### ðŸ“Š Throughput Performance
        | Metric | Value |
        |--------|-------|
        | **Target Rate** | ${{ steps.metrics.outputs.target_rate }} req/s |
        | **Actual Rate** | ${{ steps.metrics.outputs.actual_rate }} req/s |
        | **Success Rate** | ${{ steps.metrics.outputs.success_pct }}% |
        | **Total App Messages** | ${{ steps.metrics.outputs.total_messages }} |
        | **App â†’ DRA** | ${{ steps.metrics.outputs.app_to_dra }} |
        | **DRA â†’ App** | ${{ steps.metrics.outputs.dra_to_app }} |

        ### ðŸŽ¯ Performance Grade: **${{ steps.metrics.outputs.grade }}**

        #### Grade Scale:
        - **A** (ðŸŸ¢): â‰¥95% throughput, zero errors - Excellent!
        - **B** (ðŸŸ¡): 80-95% throughput, zero errors - Good
        - **C** (ðŸŸ ): <80% throughput, zero errors - Needs optimization
        - **F** (ðŸ”´): Any errors detected - Needs fixes

        ### âš ï¸ Error Analysis
        | Error Type | Count |
        |------------|-------|
        | **Routing Errors** | ${{ steps.metrics.outputs.routing_errors }} |
        | **Affinity Violations** | ${{ steps.metrics.outputs.affinity_violations }} |
        | **Connection Errors** | ${{ steps.metrics.outputs.connection_errors }} |

        ## Test Configuration Details

        ### Interface Distribution
        | Interface | Percentage | Purpose |
        |-----------|-----------|---------|
        | **S13** | 40% | Equipment Check (MICR) |
        | **S6a** | 40% | Authentication (AIR) |
        | **Gx** | 20% | Policy Control (CCR) |

        ### Test Infrastructure
        - **DRA Instances**: 1 (Load Generator)
        - **Gateway Instances**: 1
        - **Application Instances**: 12 (4 per interface)
        - **DRA Connections**: 4
        - **Total Connections**: 24

        ## Load Pattern
        - **Ramp-up**: 0 â†’ target rate over 10 seconds
        - **Sustained**: Target rate for remaining duration
        - **Message Types**: Actual Diameter requests (not protocol overhead)

        ## ðŸ“ Notes
        - Application messages are counted separately from protocol messages (CER/CEA, DWR/DWA)
        - Load generator sends actual MICR and AIR requests with realistic data
        - Metrics show bi-directional message flow (requests + answers)

        ## ðŸ“‚ Artifacts
        - [Performance Logs](../artifacts/performance-test-logs) - Gateway, DRA, and application logs
        - [Test Output](../artifacts/performance-test-logs/performance-output.txt) - Complete test output

        ---

        *Generated on: $(date -u '+%Y-%m-%d %H:%M:%S UTC')*
        *Workflow Run: [#${{ github.run_number }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})*
        EOF

    - name: Upload Performance Report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: performance-report.md
        retention-days: 30

    - name: Collect performance logs
      if: always()
      run: |
        cd tests/performance
        mkdir -p perf-logs

        # Copy test output
        cp performance-output.txt perf-logs/ 2>/dev/null || true

        # Collect container logs
        docker logs gateway-perf > perf-logs/gateway.log 2>&1 || true
        docker logs dra-perf > perf-logs/dra.log 2>&1 || true
        for i in {1..4}; do
          docker logs app-s13-$i > perf-logs/app-s13-$i.log 2>&1 || true
          docker logs app-s6a-$i > perf-logs/app-s6a-$i.log 2>&1 || true
          docker logs app-gx-$i > perf-logs/app-gx-$i.log 2>&1 || true
        done

    - name: Upload performance logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-logs
        path: tests/performance/perf-logs/
        retention-days: 7

    - name: Add Performance Results to Job Summary
      if: always()
      run: |
        cat performance-report.md >> $GITHUB_STEP_SUMMARY

        # Add quick stats
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Quick Stats" >> $GITHUB_STEP_SUMMARY
        echo "- Throughput: ${{ steps.metrics.outputs.actual_rate }}/${{ steps.metrics.outputs.target_rate }} req/s (${{ steps.metrics.outputs.success_pct }}%)" >> $GITHUB_STEP_SUMMARY
        echo "- Errors: ${{ steps.metrics.outputs.routing_errors }} routing, ${{ steps.metrics.outputs.connection_errors }} connection" >> $GITHUB_STEP_SUMMARY
        echo "- Grade: **${{ steps.metrics.outputs.grade }}** ${{ steps.metrics.outputs.grade_emoji }}" >> $GITHUB_STEP_SUMMARY

    - name: Comment PR with Performance Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('performance-report.md', 'utf8');

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });

    - name: Check Performance Thresholds
      run: |
        ROUTING_ERRORS=${{ steps.metrics.outputs.routing_errors }}
        CONNECTION_ERRORS=${{ steps.metrics.outputs.connection_errors }}
        SUCCESS_PCT=${{ steps.metrics.outputs.success_pct }}
        ACTUAL_RATE=${{ steps.metrics.outputs.actual_rate }}
        TARGET_RATE=${{ steps.metrics.outputs.target_rate }}
        GRADE="${{ steps.metrics.outputs.grade }}"

        echo "Performance Test Results:"
        echo "  Grade: $GRADE"
        echo "  Throughput: ${ACTUAL_RATE}/${TARGET_RATE} req/s (${SUCCESS_PCT}%)"
        echo "  Errors: ${ROUTING_ERRORS} routing, ${CONNECTION_ERRORS} connection"
        echo ""

        # Fail if there are any routing errors
        if [ "$ROUTING_ERRORS" != "0" ]; then
          echo "âŒ FAILED: Routing errors detected: $ROUTING_ERRORS"
          echo "Review logs for error details"
          exit 1
        fi

        # Fail if there are connection errors
        if [ "$CONNECTION_ERRORS" != "0" ]; then
          echo "âŒ FAILED: Connection errors detected: $CONNECTION_ERRORS"
          echo "Check DRA pool and application connections"
          exit 1
        fi

        # Warn if throughput is below 95% of target (but don't fail)
        if [ "$SUCCESS_PCT" -lt 95 ]; then
          echo "âš ï¸  WARNING: Throughput below 95% threshold (${SUCCESS_PCT}%)"
          echo "   Actual: $ACTUAL_RATE req/s"
          echo "   Target: $TARGET_RATE req/s"
          echo "   Consider:"
          echo "     - Increasing DRA connections (--dra-conns)"
          echo "     - Increasing application connections"
          echo "     - Tuning buffer sizes"
          echo ""
          echo "   This is a warning, not a failure."
        fi

        if [ "$GRADE" = "A" ]; then
          echo "âœ… Performance test PASSED with grade A - Excellent!"
        elif [ "$GRADE" = "B" ]; then
          echo "âœ… Performance test PASSED with grade B - Good"
        elif [ "$GRADE" = "C" ]; then
          echo "âš ï¸  Performance test passed with grade C - Needs optimization"
        else
          echo "âŒ FAILED: Grade F"
          exit 1
        fi

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [build, performance-test]
    if: always()

    steps:
    - name: Download performance report
      uses: actions/download-artifact@v4
      with:
        name: performance-report
      continue-on-error: true

    - name: Create Job Summary
      run: |
        echo "# Diameter Gateway CI/CD Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Job Status" >> $GITHUB_STEP_SUMMARY
        echo "- Build: ${{ needs.build.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Test: ${{ needs.performance-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f performance-report.md ]; then
          echo "## Performance Results" >> $GITHUB_STEP_SUMMARY
          cat performance-report.md >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "View full logs in the Actions artifacts" >> $GITHUB_STEP_SUMMARY
